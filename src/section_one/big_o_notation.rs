/* Big O Definition:
 *
 * Big O notation shows how much slower an algorithm gets as its input grows.
 *
 * We say that an algorithm is O(f(n)) if the
 * number of simple operations the computer has
 * to do is eventually less than a constant
 * times f(n), as n increases.
 *
 * -> f(n) could be linear (f(n) = n)
 * -> f(n) could be quadraric (f(n) = nÂ²)
 * -> f(n) could be constant (f(n) = 1)
 * -> f(n) could be something entirely different!
 *
 * When we talk about Big O, we talk about the worst case scenerio of an algorithm.
*/
